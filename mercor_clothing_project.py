# -*- coding: utf-8 -*-
"""Mercor_Clothing_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/sauravdas093/Mercor/blob/main/Mercor_Clothing_Project.ipynb
"""

## Mounting Google Drive to access csv files in Colab.

from google.colab import drive
drive.mount('/content/drive')

#Importing the pandas library as pd alias
import pandas as pd

# Reading the database of the clothing items from multiple e-commerce websites
flipkart = pd.read_csv('/content/drive/MyDrive/Mercor/flipkart.csv')
meesho = pd.read_csv('/content/drive/MyDrive/Mercor/meesho.csv')
myntra = pd.read_csv('/content/drive/MyDrive/Mercor/myntra.csv')

#Concat all the data from various e-commerce websites
clothing_df = pd.concat([flipkart, meesho, myntra],ignore_index=True)
clothing_df = clothing_df.reset_index()

#Alloting column names
clothing_df.columns = ['Id','Image_url', 'Brand', 'Product_url','Product','Image','product_url_nan']
clothing_df.drop(['Image','product_url_nan'],axis=1,inplace= True)
clothing_df.dropna(inplace=True)
result_df = clothing_df[['Id']].copy()
print(clothing_df.shape)
clothing_df.head(5)

#Import the nltk module
import nltk

#Import the WordNetLemmatizer class from the nltk.stem module
from nltk.stem import WordNetLemmatizer

#Create an instance of the WordNetLemmatizer class
wnl = WordNetLemmatizer()

#Download the WordNet dataset
nltk.download('wordnet')

#defining a function to lemmatize a text
def lemmatize(word_to_lemmatize):
  return(wnl.lemmatize(word_to_lemmatize))

#Preprocessing data and Text Normalization to remove special characters, lowercase conversion and lemmatization

clothing_df['Product'] = clothing_df['Product'].replace('\W', ' ', regex=True)
clothing_df['Product'] = clothing_df['Product'].str.lower()
clothing_df['Product'] = clothing_df['Product'].apply(lemmatize)

# Importing the Word2Vec model from the gensim library
from gensim.models import Word2Vec

# Importing the word_tokenize function from the nltk.tokenize module
from nltk.tokenize import word_tokenize

text_vector_mapping = {}

# Preprocess text (example)
# text1 = "sdfdsf sfgfdsgfd sfdgdsg pant for boys."

#Function to extract useful features from the text
def word_embedding(input_text):
    text2 = input_text
    # tokens1 = word_tokenize(text1.lower())  # Tokenize and convert to lowercase
    tokens2 = word_tokenize(text2.lower())  # Tokenize and convert to lowercase

    # Train Word2Vec model (example)
    model = Word2Vec(sentences=[tokens2], min_count=1, vector_size=100)
